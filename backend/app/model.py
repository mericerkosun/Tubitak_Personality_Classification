import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from joblib import dump, load
import os
from typing import List, Dict, Any

class PersonalityModel:
    def __init__(self):
        self.rf_model = None
        self.scaler = None
        self.model_path = "model/rf_model.joblib"
        self.scaler_path = "model/scaler.joblib"
        self.labeled_data_path = "clustered_dataset.csv"
        self.data_path = "data-final.csv"
        
        # KiÅŸilik boyutlarÄ± ve kÃ¼me etiketleri
        self.personality_dimensions = ['EXT', 'EST', 'AGR', 'CSN', 'OPN']
        self.cluster_labels = {
            0: "Analitik DÃ¼ÅŸÃ¼nÃ¼r",
            1: "Sosyal Lider", 
            2: "YaratÄ±cÄ± MaceracÄ±",
            3: "Uyumlu DestekÃ§i",
            4: "Organize PlanlayÄ±cÄ±"
        }
    
    def load(self) -> bool:
        """KaydedilmiÅŸ Random Forest modelini ve scaler'Ä± yÃ¼kle"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):
                self.rf_model = load(self.model_path)
                self.scaler = load(self.scaler_path)
                return True
            return False
        except Exception as e:
            print(f"Model yÃ¼klenirken hata: {e}")
            return False
    
    def create_labeled_dataset(self) -> bool:
        """KMeans ile clustering yaparak labeled veri seti oluÅŸtur"""
        try:
            print("ðŸ“Š Labeled veri seti oluÅŸturuluyor...")
            
            # 1. Veriyi oku
            df = pd.read_csv(self.data_path, sep="\t")
            print(f"Toplam veri sayÄ±sÄ±: {len(df)}")
            
            # 2. Eksik verileri temizle
            df = df.dropna()
            print(f"Temizlenen veri sayÄ±sÄ±: {len(df)}")
            
            # 3. Sadece 50 soruyu al
            df_questions = df.iloc[:, :50]
            
            # 4. Veriyi standardize et
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(df_questions)
            
            # 5. KMeans ile clustering yap
            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)
            
            # 6. Labeled veri setini oluÅŸtur
            labeled_df = df_questions.copy()
            labeled_df['personality_type'] = labels
            
            # 7. Labeled veri setini kaydet
            os.makedirs(os.path.dirname(self.labeled_data_path), exist_ok=True)
            labeled_df.to_csv(self.labeled_data_path, index=False)
            
            print(f"âœ… Labeled veri seti kaydedildi: {self.labeled_data_path}")
            print(f"Label daÄŸÄ±lÄ±mÄ±:")
            for label, count in pd.Series(labels).value_counts().sort_index().items():
                print(f"  {self.cluster_labels.get(label, f'KÃ¼me {label}')}: {count} kiÅŸi")
            
            return True
            
        except Exception as e:
            print(f"Labeled veri seti oluÅŸturulurken hata: {e}")
            return False
    
    def train(self) -> Dict[str, Any]:
        """Random Forest modelini eÄŸit ve kaydet"""
        try:
            # 1. Labeled veri seti var mÄ± kontrol et
            if not os.path.exists(self.labeled_data_path):
                raise Exception(f"Labeled veri seti bulunamadÄ±: {self.labeled_data_path}")
            
            # 2. Labeled veri setini yÃ¼kle
            print("ðŸ“š Labeled veri seti yÃ¼kleniyor...")
            df = pd.read_csv(self.labeled_data_path)
            
            # 3. Features ve target'Ä± ayÄ±r
            X = df.iloc[:, :50]  # Ä°lk 50 sÃ¼tun (sorular)
            y = df['ClusterLabel']  # Son sÃ¼tun (label)
            
            # 4. Train-test split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # 5. Veriyi standardize et
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # 6. Random Forest modeli oluÅŸtur ve eÄŸit
            print("ðŸŒ² Random Forest modeli eÄŸitiliyor...")
            self.rf_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            
            self.rf_model.fit(X_train_scaled, y_train)
            
            # 7. Model performansÄ±nÄ± deÄŸerlendir
            y_pred = self.rf_model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred, target_names=[
                self.cluster_labels[i] for i in range(5)
            ])
            
            # 8. Modeli ve scaler'Ä± kaydet
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            dump(self.rf_model, self.model_path)
            dump(self.scaler, self.scaler_path)
            
            print("âœ… Random Forest modeli ve scaler baÅŸarÄ±yla kaydedildi.")
            print(f"ðŸŽ¯ Model doÄŸruluÄŸu: {accuracy:.4f}")
            print(f"ðŸ“Š DetaylÄ± rapor:\n{report}")
            
            return {
                "accuracy": accuracy,
                "classification_report": report,
                "model_type": "Random Forest"
            }
            
        except Exception as e:
            raise Exception(f"Model eÄŸitimi baÅŸarÄ±sÄ±z: {e}")
    
    def predict(self, answers: List[float]) -> Dict[str, Any]:
        """Verilen cevaplara gÃ¶re kiÅŸilik tahmini yap"""
        try:
            if self.rf_model is None or self.scaler is None:
                raise ValueError("Model yÃ¼klenmemiÅŸ. Ã–nce modeli yÃ¼kleyin veya eÄŸitin.")
            
            if len(answers) != 50:
                raise ValueError("Tam olarak 50 soru cevabÄ± gereklidir")
            
            # CevaplarÄ± numpy array'e Ã§evir
            answers_array = np.array(answers).reshape(1, -1)
            
            # Veriyi standardize et
            answers_scaled = self.scaler.transform(answers_array)
            
            # Tahmin yap
            prediction = self.rf_model.predict(answers_scaled)[0]
            prediction_proba = self.rf_model.predict_proba(answers_scaled)[0]
            
            # KiÅŸilik Ã¶zelliklerini hesapla (her boyut iÃ§in ortalama)
            features = {}
            for i, dim in enumerate(self.personality_dimensions):
                start_idx = i * 10
                end_idx = start_idx + 10
                dim_scores = answers[start_idx:end_idx]
                features[dim.lower()] = float(np.mean(dim_scores))
            
            # GÃ¼ven skorlarÄ±
            confidence_scores = {}
            for i, label in enumerate(self.cluster_labels.values()):
                confidence_scores[label] = float(prediction_proba[i])
            
            return {
                "features": features,
                "prediction": self.cluster_labels.get(prediction, f"KÃ¼me {prediction}"),
                "cluster_id": int(prediction),
                "confidence": float(prediction_proba[prediction]),
                "all_probabilities": confidence_scores
            }
            
        except Exception as e:
            raise Exception(f"Tahmin hatasÄ±: {e}")
    
    def get_feature_importance(self) -> Dict[str, float]:
        """Random Forest'Ä±n feature importance'larÄ±nÄ± dÃ¶ndÃ¼r"""
        try:
            if self.rf_model is None:
                raise ValueError("Model yÃ¼klenmemiÅŸ")
            
            # Feature importance'larÄ± al
            importances = self.rf_model.feature_importances_
            
            # Soru bazÄ±nda importance
            feature_importance = {}
            for i in range(50):
                dim_idx = i // 10
                question_idx = (i % 10) + 1
                dim_name = self.personality_dimensions[dim_idx]
                feature_importance[f"{dim_name}{question_idx}"] = float(importances[i])
            
            # Boyut bazÄ±nda ortalama importance
            dimension_importance = {}
            for i, dim in enumerate(self.personality_dimensions):
                start_idx = i * 10
                end_idx = start_idx + 10
                dim_importance = np.mean(importances[start_idx:end_idx])
                dimension_importance[dim] = float(dim_importance)
            
            return {
                "question_importance": feature_importance,
                "dimension_importance": dimension_importance
            }
            
        except Exception as e:
            raise Exception(f"Feature importance hatasÄ±: {e}")